{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import jax.numpy as jnp\nfrom jax import random\nfrom flax import linen as nn\n#Here we imported all the libraries\n\n\n#Now we create a class that takes the nn module from flax.\nclass BigramModel(nn.Module):\n    vocab_size: int\n#The setup will just create all the neurons layers in the network\n    def setup(self):\n        self.embedding = nn.Embed(self.vocab_size, 32)\n        self.dense1 = nn.Dense(64)\n        self.dense2 = nn.Dense(self.vocab_size)\n#The call method is going to show how the input data is passed through the hidden layers.\n    def __call__(self, x):\n        x = self.embedding(x)\n        x = jnp.mean(x, axis=-2)\n        x = nn.relu(self.dense1(x))\n        x = self.dense2(x)\n        return x\n\n# We generate some random keys and plug in.\nrng = random.PRNGKey(0)\nmodel = BigramModel(vocab_size=100)\nx = jnp.zeros((32, 2))\nparams = model.init(rng, x)\n#From here, we display our output\noutput = model.apply(params, x)\nprint(output.shape) # (32, 100)\n","metadata":{},"execution_count":null,"outputs":[]}]}