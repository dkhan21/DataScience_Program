{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import jax\nfrom jax import lax, random, numpy as jnp\nfrom flax import linen as nn\nfrom typing import Any, Callable, Sequence\nfrom flax.core import freeze, unfreeze\n\n\n\nmist = nn.Dense(features=10)\n#create the layers and hidden layers\n\nn = 10\nx_lay = 10\nyval = 10\n\n#Generate the data\ntok1, tok2 = random.split(random.PRNGKey(0))\nx = random.normal(tok1, (10,)) # Dummy input data\nparams = mist.init(tok2, x) # Initialization call\njax.tree_util.tree_map(lambda x: x.shape, params) # Checking output shapes\n\n\n# Generate a key \nkey = random.PRNGKey(0)\nk1, k2 = random.split(key)\nW = random.normal(k1, (x_lay, yval))\n\nb = random.normal(k2, (yval,))\n#Have some setting for the BLM\ntrue_params = freeze({'params': {'bias': b, 'kernel': W}})\n\n# Creating a value for all the dimension using random.normal \nKeyVal, key_noise = random.split(k1)\nxval = random.normal(KeyVal, (n, x_lay))\nyval = jnp.dot(xval, W) + b + 0.1 * random.normal(key_noise,(n, yval))\n\n#Created a method to get the loss value for each iteration\ndef SquaredError(params, x_batched, y_batched):\n  def lossfunc(x, y):\n    pop = mist.apply(params, x)\n    return jnp.inner(y-pop, y-pop) / 2.0\n\n  #Put it in a vector and add an get the mean of the final value\n  return jnp.mean(jax.vmap(lossfunc)(x_batched,y_batched), axis=0)\n\nlearning_rate = 0.2\nlgF = jax.value_and_grad(SquaredError)\n\ndef update(params, learning_rate, grads):\n  params = jax.tree_util.tree_map(\n      \n      lambda p, g: p - learning_rate * g , params , grads )\n\n  return params\n\nprint(\"Using flax:\")\nfor i in range(101):\n  # Have an update for each iteration to get a value\n  loss_val, grads = lgF(params, xval, yval)\n  params = update(params, 0.2, grads)\n  if i % 10 == 0:\n    print('The loss result at ' + str(i) + \" = \", loss_val)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-26T03:46:28.076152Z","iopub.execute_input":"2023-04-26T03:46:28.076610Z","iopub.status.idle":"2023-04-26T03:46:32.208174Z","shell.execute_reply.started":"2023-04-26T03:46:28.076573Z","shell.execute_reply":"2023-04-26T03:46:32.206757Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Using flax:\nThe loss result at 0 =  59.25503\nThe loss result at 10 =  1.7329859\nThe loss result at 20 =  0.42431983\nThe loss result at 30 =  0.19928001\nThe loss result at 40 =  0.13186544\nThe loss result at 50 =  0.10130804\nThe loss result at 60 =  0.083989404\nThe loss result at 70 =  0.07311896\nThe loss result at 80 =  0.065888725\nThe loss result at 90 =  0.06083995\nThe loss result at 100 =  0.057132434\n","output_type":"stream"}]}]}