{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_1MuVtxW_qi",
        "outputId": "7e1633cd-06a3-4be1-e600-a5dedf905bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-308.6217    -107.54669    488.0352    -463.51337     55.587654\n",
            "   417.65076   -484.59384    547.15356     -6.9666758  280.2054   ]]\n"
          ]
        }
      ],
      "source": [
        "from numpy.core.numeric import outer\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "isi = 784  # These are the number of input data(features)\n",
        "his = 280  # These are the number of hidden layers\n",
        "outs = 10  # These are the number of outputs\n",
        "\n",
        "#Here we are creating and setting the biases for the MLP\n",
        "key = jax.random.PRNGKey(0)\n",
        "w1 = jax.random.normal(key, (isi, his))\n",
        "b1 = jax.random.normal(key, (his,))\n",
        "w2 = jax.random.normal(key, (his, outs))\n",
        "b2 = jax.random.normal(key, (outs,))\n",
        "\n",
        "#Now here we are defining the loss function to determine the loss differnce\n",
        "def loss(params, inputs, targets):\n",
        "    prediction = mlp_predict(params, inputs)\n",
        "    return jnp.mean((prediction - targets)**2)\n",
        "\n",
        "# Here is a foward function that reshapes the data and then it copmutes the hidden layers\n",
        "def forward(x):\n",
        "\n",
        "    x = jnp.reshape(x, (x.shape[0], -1))\n",
        "\n",
        "    hidd = jnp.dot(x, w1) + b1\n",
        "    hidd = jax.nn.relu(hidd)\n",
        "#Here it is computing the output layer for the output value\n",
        "    output = jnp.dot(hidd, w2) + b2\n",
        "    \n",
        "    return output\n",
        "#Initialize a random image\n",
        "x = jax.random.normal(key, (1, 28, 28, 1))\n",
        "#Here is a output of the MLP\n",
        "out = forward(x)\n",
        "\n",
        "# print the output shape\n",
        "print(out)  # (1, 10)"
      ]
    }
  ]
}